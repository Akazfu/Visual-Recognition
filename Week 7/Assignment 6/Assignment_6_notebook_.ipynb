{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"Assignment_6_notebook_.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YZRyyQC-3TQk","colab_type":"text"},"source":["**Import** *and* setup some auxiliary functions"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vyraYYiQ7z6M","colab":{}},"source":["# Don't edit this cell\n","import os\n","import timeit\n","import time\n","import numpy as np\n","from collections import OrderedDict\n","from pprint import pformat\n","from tqdm import tqdm\n","from google.colab import drive\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.init as init\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","from torch.utils.data.sampler import *\n","from torchvision import transforms, datasets\n","\n","torch.multiprocessing.set_sharing_strategy('file_system')\n","cudnn.benchmark = True\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4gThorCq3ypS","colab":{}},"source":["# TODO: Main model definition + any utilities such as weight initialization or custom layers, ADD DROPOUT, BATCHNORM, SKIP CONNECTION,\n","class Net(nn.Module):\n","  def __init__(self):\n","    super(Net, self).__init__()\n","    self.conv1 = nn.Conv2d(3, 52, kernel_size=5)       \n","    self.conv2 = nn.Conv2d(52, 146, kernel_size=5)      \n","    self.fc1 = nn.Linear(146 * 5 * 5, 840) \n","    self.fc2 = nn.Linear(840, 520)\n","    self.fc3 = nn.Linear(520, 10)\n","    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","    \n","    self.bn1 = nn.BatchNorm2d(146)\n","    self.bn2 = nn.BatchNorm1d(840)\n","    self.bn3 = nn.BatchNorm1d(520)\n","    self.dp = nn.Dropout(p=0.2)\n","    # Skip Connections?: to match the residual size and the input size, we need extra bn and conv2d\n","    self.sc = nn.Conv2d(3, 52, kernel_size=5)\n","    self.bnsc = nn.BatchNorm2d(52)\n","    \n","    # Xavier initialization\n","    torch.nn.init.xavier_normal_(self.conv1.weight)\n","    torch.nn.init.xavier_normal_(self.conv2.weight)\n","    torch.nn.init.xavier_normal_(self.fc1.weight)\n","    torch.nn.init.xavier_normal_(self.fc2.weight)\n","    torch.nn.init.xavier_normal_(self.fc3.weight)\n","    \n","  def forward(self, x):\n","    res = x\n","    x = F.relu(self.conv1(x))\n","    x += self.bnsc(self.sc(res))\n","    x = self.pool(x)\n","    x = self.pool(F.relu(self.bn1(self.conv2(x))))\n","    \n","    x = x.view(-1, 146 * 5 * 5)\n","    x = F.relu(self.bn2(self.fc1(x)))\n","    x = self.dp(F.relu(self.bn3(self.fc2(x))))\n","    x = F.log_softmax(self.fc3(x))\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1nxtqABv7fxm","colab_type":"code","colab":{}},"source":["# TODO: Cifar-10 dataloading\n","def load_data(config):\n","    \"\"\"\n","    Load cifar-10 dataset using torchvision, take the last 10k of the training data to be validation data\n","    \"\"\"\n","    transform_train = transforms.Compose([\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","    ])\n","\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","    ])\n","    \n","    # Import batch_size from config\n","    batch_size = config['batch_size']\n","    \n","    # Setup datasets\n","    CIFAR10_training = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","    CIFAR10_test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","    \n","    # create a training and a validation set by spliting training dataset to first 45000 and last 5000 indices\n","    num_train = len(CIFAR10_training)\n","    indices = list(range(num_train))\n","    train_idx, valid_idx = indices[:45000], indices[45000:]\n","    \n","    # Setup dataloaders, batch_size for the test loader should be 1\n","    train_sampler = SubsetRandomSampler(train_idx) \n","    validation_sampler = SubsetRandomSampler(valid_idx)\n","    \n","    train_dataloader = torch.utils.data.DataLoader(CIFAR10_training,batch_size=batch_size, sampler=train_sampler, num_workers=2)\n","    valid_dataloader = torch.utils.data.DataLoader(CIFAR10_training,batch_size=batch_size, sampler=validation_sampler, num_workers=2)\n","    test_dataloader = torch.utils.data.DataLoader(CIFAR10_test_set,batch_size=1, shuffle=False, num_workers=2)\n","    \n","    \n","    return train_dataloader, valid_dataloader, test_dataloader"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q3GcXjw5emv0","colab_type":"code","colab":{}},"source":["# TODO : Main trainig + validation, returns the final model, save your best checkpoint based on the best validation accuracy\n","def train(train_dataloader, valid_dataloader, device, config):\n","  # Import parameters from config\n","  lr = config['lr']\n","  momentum = config['momentum']\n","  num_epochs = config['num_epochs']\n","  regular_constant = config['regular_constant']\n","  log_interval = config['log_interval']\n","  \n","  # Setup model, loss function, optimizer, best accuracy and current accuracy\n","  model = Net().to(device)\n","  best_acc, curr_acc = 0, 0\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=regular_constant)\n","  \n","  # Per Iterations training + validation\n","  for epoch in range(1, num_epochs + 1):\n","    # Training\n","    for batch_idx, (data, target) in enumerate(train_dataloader):\n","      data = data.to(device)\n","      target = target.to(device)\n","      # Forward -> Backprop -> Optimize\n","      optimizer.zero_grad()\n","      output = model(data)\n","      loss = criterion(output, target)\n","      loss.backward()\n","      optimizer.step()\n","      \n","      if batch_idx % log_interval == 0:\n","        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","          epoch, batch_idx * len(data), len(train_dataloader.sampler),\n","          100. * batch_idx / len(train_dataloader), loss.item()))\n","    \n","    # Validation with 5000 dataset\n","    validation_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","      for data, target in valid_dataloader:\n","        data = data.to(device)\n","        target = target.to(device)\n","        output = model(data)\n","        validation_loss += criterion(output, target).item()\n","        pred = output.data.max(1, keepdim=True)[1]\n","        correct += pred.eq(target.data.view_as(pred)).sum()\n","    validation_loss /= len(valid_dataloader.sampler)\n","    print('\\nValidation set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        validation_loss, correct, len(valid_dataloader.sampler),\n","        100. * correct / len(valid_dataloader.sampler)))\n","    \n","    # Save the checkpoint with the best accuracy\n","    curr_acc = 100. * correct / len(valid_dataloader.sampler)\n","    if curr_acc >= best_acc:\n","      save_checkpoint(model)\n","      best_acc = curr_acc\n","      print('(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\\n')\n","      \n","  \n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_vrzcP0X_fmt","colab_type":"code","colab":{}},"source":["# A simple function for save model to the checkpoint\n","def save_checkpoint(model):\n","  #drive.mount('/content/gdrive/', force_remount=True)\n","  torch.save(model.state_dict(), '/content/gdrive/My Drive/checkpoint/ckpt.pth')\n","  \n","def save_model_colab_for_submission(model):  # if you are running on colab\n","  #drive.mount('/content/gdrive/', force_remount=True)\n","  torch.save(model.to(torch.device(\"cpu\")), '/content/gdrive/My Drive/model.pt') # you will find the model in your home drive\n","  \n","def save_model_local_for_submission(model):  # if you are running on your local machine\n","  torch.save(model.to(torch.device(\"cpu\")), 'model.pt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MV6N9Y-kCfWy","colab_type":"code","colab":{}},"source":["#TODO: Implement testing\n","def test(model, test_dataloader, device):\n","  correct = 0\n","  total = 0\n","  \n","  test_loss = 0\n","  criterion = nn.CrossEntropyLoss()\n","  \n","  with torch.no_grad():\n","    for data, target in test_dataloader:\n","      data = data.to(device)\n","      target = target.to(device)\n","      output = model(data)\n","      test_loss += criterion(output, target).item()\n","      pred = output.data.max(1, keepdim=True)[1]\n","      correct += pred.eq(target.data.view_as(pred)).sum()\n","      \n","  test_loss /= len(test_dataloader.dataset)\n","  total = len(test_dataloader.dataset)\n","  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","    test_loss, correct, len(test_dataloader.dataset),\n","    100. * correct / len(test_dataloader.dataset)))\n","\n","  return 100.*correct/total, correct, total"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9dIUkH6e5T2","colab_type":"code","colab":{}},"source":["def run():\n","  # Mount the google drive\n","  drive.mount('/content/gdrive/', force_remount=True)\n","  \n","  # set parameters cifar10\n","  config = {\n","        'lr': 1e-2,\n","        'num_epochs': 20,\n","        'batch_size': 128,\n","        'num_classes': 10,\n","        'momentum': 0.9,\n","        'regular_constant': 1e-4,\n","        'log_interval': 100,\n","       }\n","    \n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  \n","  train_dataloader, valid_dataloader, test_dataloader = load_data(config)\n","  \n","  model = train(train_dataloader, valid_dataloader, device, config)\n","  \n","  # Testing and saving for submission\n","  device = torch.device(\"cpu\")\n","  \n","  # Modified for google drive use\n","  #assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n","  #checkpoint = torch.load('./checkpoint/ckpt.pth')\n","  \n","  checkpoint = torch.load('/content/gdrive/My Drive/checkpoint/ckpt.pth')\n","  model.load_state_dict(checkpoint)\n","  model.eval()\n","  \n","  start_time = timeit.default_timer()\n","  test_acc, test_correct, test_total = test(model.to(device), test_dataloader, device)\n","  end_time = timeit.default_timer()\n","  test_time = (end_time - start_time)\n","  \n","  save_model_colab_for_submission(model)\n","\n","  return test_acc, test_correct, test_time\n","\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GNNgL7C7cQq-","colab_type":"text"},"source":["Main loop. Run time and total score will be shown below."]},{"cell_type":"code","metadata":{"id":"Qf9iL8S_cQrB","colab_type":"code","outputId":"61075203-62cc-4a80-9259-d5f8685882ca","executionInfo":{"status":"ok","timestamp":1572021496015,"user_tz":360,"elapsed":367883,"user":{"displayName":"Ziming Fu","photoUrl":"","userId":"13730860658836574087"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Don't edit this cell\n","def compute_score(acc, min_thres=65, max_thres=80):\n","  # Your Score thresholds\n","  if acc <= min_thres:\n","      base_score = 0.0\n","  elif acc >= max_thres:\n","      base_score = 100.0\n","  else:\n","      base_score = float(acc - min_thres) / (max_thres - min_thres) * 100\n","  return base_score\n","\n","def main():\n","    \n","    accuracy, correct, run_time = run()\n","    \n","    score = compute_score(accuracy)\n","    \n","    result = OrderedDict(correct=correct,\n","                         accuracy=accuracy,\n","                         run_time=run_time,\n","                         score=score)\n","    \n","    with open('result.txt', 'w') as f:\n","        f.writelines(pformat(result, indent=4))\n","    print(\"\\nResult:\\n\", pformat(result, indent=4))\n","\n","\n","main()"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 1 [0/45000 (0%)]\tLoss: 2.919630\n","Train Epoch: 1 [12800/45000 (28%)]\tLoss: 1.654056\n","Train Epoch: 1 [25600/45000 (57%)]\tLoss: 1.393146\n","Train Epoch: 1 [38400/45000 (85%)]\tLoss: 1.371235\n","\n","Validation set: Avg. loss: 0.0106, Accuracy: 2627/5000 (53%)\n","\n","(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\n","\n","Train Epoch: 2 [0/45000 (0%)]\tLoss: 1.253615\n","Train Epoch: 2 [12800/45000 (28%)]\tLoss: 1.053297\n","Train Epoch: 2 [25600/45000 (57%)]\tLoss: 1.320375\n","Train Epoch: 2 [38400/45000 (85%)]\tLoss: 1.132117\n","\n","Validation set: Avg. loss: 0.0094, Accuracy: 2913/5000 (58%)\n","\n","(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\n","\n","Train Epoch: 3 [0/45000 (0%)]\tLoss: 1.204578\n","Train Epoch: 3 [12800/45000 (28%)]\tLoss: 1.289159\n","Train Epoch: 3 [25600/45000 (57%)]\tLoss: 1.255394\n","Train Epoch: 3 [38400/45000 (85%)]\tLoss: 1.064853\n","\n","Validation set: Avg. loss: 0.0082, Accuracy: 3171/5000 (63%)\n","\n","(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\n","\n","Train Epoch: 4 [0/45000 (0%)]\tLoss: 0.826336\n","Train Epoch: 4 [12800/45000 (28%)]\tLoss: 1.110394\n","Train Epoch: 4 [25600/45000 (57%)]\tLoss: 1.013150\n","Train Epoch: 4 [38400/45000 (85%)]\tLoss: 0.973957\n","\n","Validation set: Avg. loss: 0.0077, Accuracy: 3317/5000 (66%)\n","\n","(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\n","\n","Train Epoch: 5 [0/45000 (0%)]\tLoss: 0.799397\n","Train Epoch: 5 [12800/45000 (28%)]\tLoss: 0.953847\n","Train Epoch: 5 [25600/45000 (57%)]\tLoss: 1.149900\n","Train Epoch: 5 [38400/45000 (85%)]\tLoss: 0.956831\n","\n","Validation set: Avg. loss: 0.0072, Accuracy: 3422/5000 (68%)\n","\n","(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\n","\n","Train Epoch: 6 [0/45000 (0%)]\tLoss: 0.893899\n","Train Epoch: 6 [12800/45000 (28%)]\tLoss: 0.779651\n","Train Epoch: 6 [25600/45000 (57%)]\tLoss: 0.828171\n","Train Epoch: 6 [38400/45000 (85%)]\tLoss: 0.918718\n","\n","Validation set: Avg. loss: 0.0070, Accuracy: 3503/5000 (70%)\n","\n","(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\n","\n","Train Epoch: 7 [0/45000 (0%)]\tLoss: 1.098005\n","Train Epoch: 7 [12800/45000 (28%)]\tLoss: 0.871679\n","Train Epoch: 7 [25600/45000 (57%)]\tLoss: 0.814518\n","Train Epoch: 7 [38400/45000 (85%)]\tLoss: 0.841971\n","\n","Validation set: Avg. loss: 0.0067, Accuracy: 3554/5000 (71%)\n","\n","(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\n","\n","Train Epoch: 8 [0/45000 (0%)]\tLoss: 0.851147\n","Train Epoch: 8 [12800/45000 (28%)]\tLoss: 0.730422\n","Train Epoch: 8 [25600/45000 (57%)]\tLoss: 0.838415\n","Train Epoch: 8 [38400/45000 (85%)]\tLoss: 0.825023\n","\n","Validation set: Avg. loss: 0.0066, Accuracy: 3557/5000 (71%)\n","\n","(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\n","\n","Train Epoch: 9 [0/45000 (0%)]\tLoss: 0.673689\n","Train Epoch: 9 [12800/45000 (28%)]\tLoss: 0.760488\n","Train Epoch: 9 [25600/45000 (57%)]\tLoss: 0.848829\n","Train Epoch: 9 [38400/45000 (85%)]\tLoss: 0.600914\n","\n","Validation set: Avg. loss: 0.0064, Accuracy: 3584/5000 (72%)\n","\n","(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\n","\n","Train Epoch: 10 [0/45000 (0%)]\tLoss: 0.777645\n","Train Epoch: 10 [12800/45000 (28%)]\tLoss: 0.596806\n","Train Epoch: 10 [25600/45000 (57%)]\tLoss: 0.726503\n","Train Epoch: 10 [38400/45000 (85%)]\tLoss: 0.866578\n","\n","Validation set: Avg. loss: 0.0061, Accuracy: 3695/5000 (74%)\n","\n","(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\n","\n","Train Epoch: 11 [0/45000 (0%)]\tLoss: 0.480468\n","Train Epoch: 11 [12800/45000 (28%)]\tLoss: 0.756518\n","Train Epoch: 11 [25600/45000 (57%)]\tLoss: 0.994790\n","Train Epoch: 11 [38400/45000 (85%)]\tLoss: 0.687221\n","\n","Validation set: Avg. loss: 0.0065, Accuracy: 3660/5000 (73%)\n","\n","Train Epoch: 12 [0/45000 (0%)]\tLoss: 0.735151\n","Train Epoch: 12 [12800/45000 (28%)]\tLoss: 0.678004\n","Train Epoch: 12 [25600/45000 (57%)]\tLoss: 0.577776\n","Train Epoch: 12 [38400/45000 (85%)]\tLoss: 0.694920\n","\n","Validation set: Avg. loss: 0.0058, Accuracy: 3756/5000 (75%)\n","\n","(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\n","\n","Train Epoch: 13 [0/45000 (0%)]\tLoss: 0.639038\n","Train Epoch: 13 [12800/45000 (28%)]\tLoss: 0.751361\n","Train Epoch: 13 [25600/45000 (57%)]\tLoss: 0.557911\n","Train Epoch: 13 [38400/45000 (85%)]\tLoss: 0.653519\n","\n","Validation set: Avg. loss: 0.0057, Accuracy: 3724/5000 (74%)\n","\n","Train Epoch: 14 [0/45000 (0%)]\tLoss: 0.634840\n","Train Epoch: 14 [12800/45000 (28%)]\tLoss: 0.597099\n","Train Epoch: 14 [25600/45000 (57%)]\tLoss: 0.796304\n","Train Epoch: 14 [38400/45000 (85%)]\tLoss: 0.673569\n","\n","Validation set: Avg. loss: 0.0058, Accuracy: 3690/5000 (74%)\n","\n","Train Epoch: 15 [0/45000 (0%)]\tLoss: 0.625586\n","Train Epoch: 15 [12800/45000 (28%)]\tLoss: 0.541901\n","Train Epoch: 15 [25600/45000 (57%)]\tLoss: 0.550334\n","Train Epoch: 15 [38400/45000 (85%)]\tLoss: 0.642759\n","\n","Validation set: Avg. loss: 0.0057, Accuracy: 3807/5000 (76%)\n","\n","(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\n","\n","Train Epoch: 16 [0/45000 (0%)]\tLoss: 0.571526\n","Train Epoch: 16 [12800/45000 (28%)]\tLoss: 0.623000\n","Train Epoch: 16 [25600/45000 (57%)]\tLoss: 0.646609\n","Train Epoch: 16 [38400/45000 (85%)]\tLoss: 0.664928\n","\n","Validation set: Avg. loss: 0.0057, Accuracy: 3822/5000 (76%)\n","\n","(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\n","\n","Train Epoch: 17 [0/45000 (0%)]\tLoss: 0.525118\n","Train Epoch: 17 [12800/45000 (28%)]\tLoss: 0.544526\n","Train Epoch: 17 [25600/45000 (57%)]\tLoss: 0.590966\n","Train Epoch: 17 [38400/45000 (85%)]\tLoss: 0.591502\n","\n","Validation set: Avg. loss: 0.0055, Accuracy: 3806/5000 (76%)\n","\n","Train Epoch: 18 [0/45000 (0%)]\tLoss: 0.603317\n","Train Epoch: 18 [12800/45000 (28%)]\tLoss: 0.485942\n","Train Epoch: 18 [25600/45000 (57%)]\tLoss: 0.591119\n","Train Epoch: 18 [38400/45000 (85%)]\tLoss: 0.695940\n","\n","Validation set: Avg. loss: 0.0054, Accuracy: 3876/5000 (78%)\n","\n","(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\n","\n","Train Epoch: 19 [0/45000 (0%)]\tLoss: 0.532002\n","Train Epoch: 19 [12800/45000 (28%)]\tLoss: 0.415918\n","Train Epoch: 19 [25600/45000 (57%)]\tLoss: 0.521306\n","Train Epoch: 19 [38400/45000 (85%)]\tLoss: 0.573784\n","\n","Validation set: Avg. loss: 0.0053, Accuracy: 3901/5000 (78%)\n","\n","(Current accuracy >= Best accuracy) ===> Model has been updated to the checkpoint\n","\n","Train Epoch: 20 [0/45000 (0%)]\tLoss: 0.428331\n","Train Epoch: 20 [12800/45000 (28%)]\tLoss: 0.460444\n","Train Epoch: 20 [25600/45000 (57%)]\tLoss: 0.511230\n","Train Epoch: 20 [38400/45000 (85%)]\tLoss: 0.511418\n","\n","Validation set: Avg. loss: 0.0056, Accuracy: 3839/5000 (77%)\n","\n","\n","Test set: Avg. loss: 0.5667, Accuracy: 8056/10000 (81%)\n","\n","\n","Result:\n"," OrderedDict([   ('correct', tensor(8056)),\n","                ('accuracy', tensor(80.5600)),\n","                ('run_time', 59.430272005999996),\n","                ('score', 100.0)])\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm1d. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"}]}]}