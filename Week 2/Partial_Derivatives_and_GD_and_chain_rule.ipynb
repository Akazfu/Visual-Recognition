{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Partial_Derivatives_and_GD_and_chain_rule.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYz-EHRRjR5s",
        "colab_type": "text"
      },
      "source": [
        "# Multivariate function and its derivative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBdw1ckMUomp",
        "colab_type": "code",
        "outputId": "5dedbd11-441a-45da-92fa-da8bcbf30320",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
        "f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
        "\n",
        "print(\"Vector variable x:\",x.data)\n",
        "\n",
        "print(\"Function f at x:\",f.data)\n",
        "\n",
        "# compute gradient of f at x\n",
        "g = torch.autograd.grad(f,x)\n",
        "\n",
        "print(\"Gradient of f at x:\",g[0].data)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vector variable x: tensor([ 3., -1.,  0.,  1.])\n",
            "Function f at x: tensor(215.)\n",
            "Gradient of f at x: tensor([ 306., -144.,   -2., -310.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB85808hjuOK",
        "colab_type": "text"
      },
      "source": [
        "#Gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYaX2d0PYmOu",
        "colab_type": "code",
        "outputId": "02419984-cedb-49ed-951d-18f95ed833f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
        "  \n",
        "steplength = 1e-3 # for gradient descent\n",
        "for i in range(1000):\n",
        "  # function\n",
        "  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
        "  # compute grdaient\n",
        "  g = torch.autograd.grad(f,x)\n",
        "  # adjust variable\n",
        "  x = x - steplength*g[0]\n",
        "  if i%100==0:\n",
        "    print(\"Current variable value:\",x.detach().numpy(),\"Current function value:\", f.item())\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current variable value: [ 2.694e+00 -8.560e-01  2.000e-03  1.310e+00] Current function value: 215.0\n",
            "Current variable value: [ 1.7120734  -0.15217544  0.43262678  1.239527  ] Current function value: 4.898355960845947\n",
            "Current variable value: [ 1.3354341  -0.12037495  0.38850418  0.923193  ] Current function value: 2.40033221244812\n",
            "Current variable value: [ 1.0786515  -0.0987131   0.3482537   0.71352595] Current function value: 1.2607851028442383\n",
            "Current variable value: [ 0.89774257 -0.08322652  0.31416288  0.57114387] Current function value: 0.708449125289917\n",
            "Current variable value: [ 0.7665225  -0.07183251  0.28545532  0.47160053] Current function value: 0.42393937706947327\n",
            "Current variable value: [ 0.6686587  -0.06322152  0.26128545  0.39994755] Current function value: 0.2685073912143707\n",
            "Current variable value: [ 0.593754   -0.0565507   0.2408716   0.34689713] Current function value: 0.17878548800945282\n",
            "Current variable value: [ 0.53504807 -0.05126574  0.2235377   0.30656764] Current function value: 0.12432404607534409\n",
            "Current variable value: [ 0.48804614 -0.04699376  0.208721    0.27515563] Current function value: 0.08974049240350723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itsqyBm0j22X",
        "colab_type": "text"
      },
      "source": [
        "#Gradient descent using PyTorch's optmization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfW8cdRDiZYz",
        "colab_type": "code",
        "outputId": "78a38be2-55f0-4e8d-bccf-92a0f1536e4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
        "  \n",
        "optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n",
        "\n",
        "for i in range(100):\n",
        "  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
        "  optimizer.zero_grad()\n",
        "  f.backward()\n",
        "  optimizer.step()\n",
        "  if i%10==0:\n",
        "    print(\"Current variable value:\",x.detach().numpy(),\"Current function value:\", f.item())\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current variable value: [-27.6       13.400001   0.2       32.      ] Current function value: 215.0\n",
            "Current variable value: [nan nan nan nan] Current function value: nan\n",
            "Current variable value: [nan nan nan nan] Current function value: nan\n",
            "Current variable value: [nan nan nan nan] Current function value: nan\n",
            "Current variable value: [nan nan nan nan] Current function value: nan\n",
            "Current variable value: [nan nan nan nan] Current function value: nan\n",
            "Current variable value: [nan nan nan nan] Current function value: nan\n",
            "Current variable value: [nan nan nan nan] Current function value: nan\n",
            "Current variable value: [nan nan nan nan] Current function value: nan\n",
            "Current variable value: [nan nan nan nan] Current function value: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9OZiFkLnNhT",
        "colab_type": "text"
      },
      "source": [
        "#Chain rule of derivative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80oOHqfynMjr",
        "colab_type": "code",
        "outputId": "f0565fd7-674c-45d2-ae6b-688f786fa879",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "z = Variable(torch.tensor([1.0,-1.0]),requires_grad=True)\n",
        "\n",
        "print(\"Variable z:\",z)\n",
        "\n",
        "def compute_x(z):\n",
        "  x = torch.zeros(4)\n",
        "  x[0] = z[0] - z[1]\n",
        "  x[1] = z[0]**2\n",
        "  x[2] = z[1]**2\n",
        "  x[3] = z[0]**2+z[0]*z[1]\n",
        "  return x\n",
        "\n",
        "x = compute_x(z)\n",
        "print(\"function x:\",x)\n",
        "\n",
        "def compute_f(x):\n",
        "  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
        "  return f\n",
        "\n",
        "f = compute_f(x)\n",
        "print(\"function f:\",f)\n",
        "print(\"\")\n",
        "# \n",
        "# Let's compute gradient of f with respect to x\n",
        "g_x = torch.autograd.grad(f,x,retain_graph=True,create_graph=True)\n",
        "# Now compute Jacobian of x with respect to z and multiply with g_x to use chain rule\n",
        "g_z = torch.autograd.grad(x,z,g_x,retain_graph=True) \n",
        "\n",
        "# But PyTorch can compute derivative of f with respect to z directly - this is the amazing capability!\n",
        "g = torch.autograd.grad(f,z)\n",
        "\n",
        "print(\"Gradient by chain rule:\",g_z[0])\n",
        "print(\"Gradient by PyTorch:\",g[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variable z: tensor([ 1., -1.], requires_grad=True)\n",
            "function x: tensor([2., 1., 1., 0.], grad_fn=<CopySlices>)\n",
            "function f: tensor(310., grad_fn=<AddBackward0>)\n",
            "\n",
            "Gradient by chain rule: tensor([ 486., -710.])\n",
            "Gradient by PyTorch: tensor([ 486., -710.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyEZ0Qy_zIFN",
        "colab_type": "text"
      },
      "source": [
        "#Optimization by gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igcWXotzzWfz",
        "colab_type": "code",
        "outputId": "84983fa9-aa13-4331-8163-7a3296326664",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "steplength = 1e-3 # for gradient descent\n",
        "for i in range(1000):\n",
        "  # function\n",
        "  f = compute_f(compute_x(z))\n",
        "  # Compute gradient of f with respect to z directly\n",
        "  # PyTorch takes care of chain rule of derivatives\n",
        "  g = torch.autograd.grad(f,z) \n",
        "  # adjust variable\n",
        "  z = z - steplength*g[0]\n",
        "  if i%100==0:\n",
        "    print(\"Current variable value:\",z.detach().numpy(),\"Current function value:\", f.item())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current variable value: [ 0.51399994 -0.28999996] Current function value: 310.0\n",
            "Current variable value: [ 0.02995771 -0.08415731] Current function value: 0.017649315297603607\n",
            "Current variable value: [ 0.0033195  -0.06245444] Current function value: 0.0046606045216321945\n",
            "Current variable value: [-0.00774205 -0.05076593] Current function value: 0.0019731733482331038\n",
            "Current variable value: [-0.0138273  -0.04306844] Current function value: 0.0009890968212857842\n",
            "Current variable value: [-0.01762226 -0.03758883] Current function value: 0.0005377104971557856\n",
            "Current variable value: [-0.02015965 -0.03353591] Current function value: 0.0003060725866816938\n",
            "Current variable value: [-0.02193012 -0.0304734 ] Current function value: 0.0001794985291780904\n",
            "Current variable value: [-0.02319964 -0.02812895] Current function value: 0.00010766605555545539\n",
            "Current variable value: [-0.02412602 -0.0263188 ] Current function value: 6.590562406927347e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1MbO8BG0Gzk",
        "colab_type": "text"
      },
      "source": [
        "#And of course optimization using PyTorch's gradient descent optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVsIisPK0RCk",
        "colab_type": "code",
        "outputId": "30c7bbcb-a6d5-4356-dc88-2e50363f7861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "z = Variable(torch.tensor([1.0,-1.0]),requires_grad=True)\n",
        "  \n",
        "optimizer = torch.optim.SGD([z], lr=1e-3, momentum=0.9) # create an optimizer that will do gradient descent optimization\n",
        "\n",
        "for i in range(100):\n",
        "  f = compute_f(compute_x(z))\n",
        "  optimizer.zero_grad()\n",
        "  f.backward()\n",
        "  optimizer.step()\n",
        "  if i%10==0:\n",
        "    print(\"Current variable value:\",z.detach().numpy(),\"Current function value:\", f.item())\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current variable value: [ 0.514      -0.28999996] Current function value: 310.0\n",
            "Current variable value: [ 0.29414672 -0.63746554] Current function value: 78.43021392822266\n",
            "Current variable value: [0.303851   0.03234617] Current function value: 18.59418487548828\n",
            "Current variable value: [0.24348229 0.30544168] Current function value: 0.07254856824874878\n",
            "Current variable value: [-0.0413984   0.35884088] Current function value: 0.3099023699760437\n",
            "Current variable value: [-0.22963157  0.17217131] Current function value: 0.4768257737159729\n",
            "Current variable value: [-0.05684688 -0.02945037] Current function value: 0.0002854902413673699\n",
            "Current variable value: [ 0.00743173 -0.10030661] Current function value: 0.011820731684565544\n",
            "Current variable value: [ 0.0100228  -0.10663033] Current function value: 0.017412500455975533\n",
            "Current variable value: [-0.01059541 -0.08887993] Current function value: 0.007626336999237537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTUnFP6tjemZ",
        "colab_type": "text"
      },
      "source": [
        "#Hessian computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChfNnPqrVTII",
        "colab_type": "code",
        "outputId": "e9e53eac-6dc7-4371-9f50-0f0c3d50af11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
        "f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
        "\n",
        "# PyTorch does not compute Hessian (second order derivatives) directly\n",
        "# PyTorch can compute Jacobian vector product \n",
        "# We can use Jacobian vector product to compute Hessian\n",
        "\n",
        "# Step 1: compute gradient\n",
        "g = torch.autograd.grad(f,x,retain_graph=True,create_graph=True) # compute gradient with two important flags on\n",
        "\n",
        "# Step 2: Use product of Jacobian of g and columns of identity matrix to compute hessian of f\n",
        "eye = torch.eye(4) # 4-by-4 identity matrix\n",
        "H = torch.stack([torch.autograd.grad(g,x,eye[:,i],retain_graph=True)[0] for i in range(4)]) # hessian\n",
        "\n",
        "print(\"Hessian:\",H.data)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hessian: tensor([[ 482.,   20.,    0., -480.],\n",
            "        [  20.,  212.,  -24.,    0.],\n",
            "        [   0.,  -24.,   58.,  -10.],\n",
            "        [-480.,    0.,  -10.,  490.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDghJQA9kf6L",
        "colab_type": "text"
      },
      "source": [
        "#Newton's method (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynGB444bke1c",
        "colab_type": "code",
        "outputId": "de509019-83cc-48a1-8ddc-8e374d0d3858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Newton's optimization for an example - Powell Function (https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf)\n",
        "# Minimize Powell function: f(x1,x2,x3,x4) = (x1+10x2)^2 + 5(x3-x4)^2 + (x2-2x3)^4 + 10(x1-x4)^4\n",
        "\n",
        "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
        "f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
        "\n",
        "def LineSearch(x,d):\n",
        "  minstep = 1.0\n",
        "  minval=1e10\n",
        "  for i in range(10):\n",
        "    step = (i+1)/10.0\n",
        "    xp = x.data + step*d.data\n",
        "    fval = (xp[0]+10.0*xp[1])**2 + 5.0*(xp[2]-xp[3])**2 + (xp[1]-2.0*xp[2])**4 + 10.0*(xp[0]-xp[3])**4\n",
        "    if fval < minval:\n",
        "      minval = fval\n",
        "      minstep = step\n",
        "  return minstep\n",
        "\n",
        "eye = torch.eye(4)\n",
        "\n",
        "for itr in range(10):\n",
        "  # Step 1: compute Newton direction d\n",
        "  g = torch.autograd.grad(f,x,retain_graph=True,create_graph=True) # gradient\n",
        "  H = torch.stack([torch.autograd.grad(g,x,eye[:,i],retain_graph=True)[0] for i in range(4)]) # hessian\n",
        "  d = torch.solve(-g[0].unsqueeze(1), H)[0].t().squeeze() # solve Newton system\n",
        "  \n",
        "  # Step 2: update x with Newton direction d\n",
        "  step_length = LineSearch(x,d)\n",
        "  x.data += step_length*d.data # often step_length is set as 1.0\n",
        "  print(x.data)\n",
        "  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
        "  print(f.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 1.5873, -0.1587,  0.2540,  0.2540])\n",
            "tensor(31.8025)\n",
            "tensor([ 1.0582, -0.1058,  0.1693,  0.1693])\n",
            "tensor(6.2820)\n",
            "tensor([ 0.7055, -0.0705,  0.1129,  0.1129])\n",
            "tensor(1.2409)\n",
            "tensor([ 0.4703, -0.0470,  0.0752,  0.0752])\n",
            "tensor(0.2451)\n",
            "tensor([ 0.3135, -0.0314,  0.0502,  0.0502])\n",
            "tensor(0.0484)\n",
            "tensor([ 0.2090, -0.0209,  0.0334,  0.0334])\n",
            "tensor(0.0096)\n",
            "tensor([ 0.1394, -0.0139,  0.0223,  0.0223])\n",
            "tensor(0.0019)\n",
            "tensor([ 0.0929, -0.0093,  0.0149,  0.0149])\n",
            "tensor(0.0004)\n",
            "tensor([ 0.0619, -0.0062,  0.0099,  0.0099])\n",
            "tensor(7.3712e-05)\n",
            "tensor([ 0.0413, -0.0041,  0.0066,  0.0066])\n",
            "tensor(1.4560e-05)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9qccXCcknY8",
        "colab_type": "text"
      },
      "source": [
        "#Conjugate gradient method (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLI5FGLeQxUO",
        "colab_type": "code",
        "outputId": "4b7fadf2-98cf-427c-f5de-1a2001fe8aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# Newton method using conjugate gradient (https://en.wikipedia.org/wiki/Conjugate_gradient_method)\n",
        "# Hessian-vector product computation needs one autograd call per iteration inside CG iteration\n",
        "# So this method never computes and stores the full hessian matrix\n",
        "# CG solver might converge faster than other general linear equation solver\n",
        "# Also, it seems to be more stable\n",
        "\n",
        "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
        "f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
        "\n",
        "for itr in range(10):\n",
        "  # Step 1: compute Newton direction d by CG method\n",
        "  g = torch.autograd.grad(f,x,retain_graph=True,create_graph=True) # gradient\n",
        "  r = -g[0].data.clone()\n",
        "  p = r.clone()\n",
        "  d = torch.tensor([0.,0.,0.,0.])\n",
        "  rsold = torch.sum(r**2)\n",
        "  for cg_itr in range(6): # cg_itr should be slightly larger length of variable - here variable length is 4\n",
        "    q = torch.autograd.grad(g,x,p,retain_graph=True)[0] # hessian-vector (Jacobian-vector) product computation by autograd\n",
        "    alpha = rsold/torch.sum(p*q)\n",
        "    d += alpha*p\n",
        "    r += -alpha*q\n",
        "    rsnew = torch.sum(r**2)\n",
        "    if rsnew<1e-10:\n",
        "      break\n",
        "    p = r + (rsnew/rsold)*p\n",
        "    rsold = rsnew\n",
        "    \n",
        "  # Step 2: update x with Newton direction d\n",
        "  step_length = LineSearch(x,d)\n",
        "  x.data += step_length*d.data # often step_length is set as 1.0\n",
        "  print(x.data)\n",
        "  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
        "  print(f.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 1.5873, -0.1587,  0.2540,  0.2540])\n",
            "tensor(31.8025)\n",
            "tensor([ 1.0582, -0.1058,  0.1693,  0.1693])\n",
            "tensor(6.2820)\n",
            "tensor([ 0.7055, -0.0705,  0.1129,  0.1129])\n",
            "tensor(1.2409)\n",
            "tensor([ 0.4703, -0.0470,  0.0752,  0.0752])\n",
            "tensor(0.2451)\n",
            "tensor([ 0.3135, -0.0314,  0.0502,  0.0502])\n",
            "tensor(0.0484)\n",
            "tensor([ 0.2090, -0.0209,  0.0334,  0.0334])\n",
            "tensor(0.0096)\n",
            "tensor([ 0.1394, -0.0139,  0.0223,  0.0223])\n",
            "tensor(0.0019)\n",
            "tensor([ 0.0929, -0.0093,  0.0149,  0.0149])\n",
            "tensor(0.0004)\n",
            "tensor([ 0.0619, -0.0062,  0.0099,  0.0099])\n",
            "tensor(7.3712e-05)\n",
            "tensor([ 0.0413, -0.0041,  0.0066,  0.0066])\n",
            "tensor(1.4561e-05)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEQYs6zvrJ0P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1480fe79-8666-4bf9-c371-588aefca8096"
      },
      "source": [
        "def compute_func(x):\n",
        "  if x[0]<=0 and x[1]>=2:\n",
        "    return x[0]**2 + x[1]**2\n",
        "  else:\n",
        "    return -2.0*x[0]\n",
        "\n",
        "x = Variable(torch.tensor([2.0,0.0]),requires_grad=True)\n",
        "\n",
        "f = compute_func(x)\n",
        "g = torch.autograd.grad(f,x)\n",
        "print(g)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([-2.,  0.]),)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIcDkF7CDAVi",
        "colab_type": "text"
      },
      "source": [
        "#Numerical Derivative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFl_NFdGC_bq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "e8cdcd14-8bf6-45e0-e078-e0a6b582ac92"
      },
      "source": [
        "x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n",
        "\n",
        "def compute_f(x):\n",
        "  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n",
        "  return f\n",
        "\n",
        "f = compute_f(x)\n",
        "\n",
        "print(\"Vector variable x:\",x.data)\n",
        "\n",
        "print(\"Function f at x:\",f.data)\n",
        "\n",
        "# compute gradient of f at x\n",
        "g = torch.autograd.grad(f,x)\n",
        "\n",
        "print(\"Gradient of f at x:\",g[0].data)\n",
        "\n",
        "num_g = torch.zeros(4)\n",
        "h=1e-3\n",
        "for i in range(4):\n",
        "  num_g[i] = compute_f(x+h*eye[i,:]) - compute_f(x-h*eye[i,:])\n",
        "\n",
        "num_g = num_g/(2.0*h)\n",
        "\n",
        "print(num_g)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vector variable x: tensor([ 3., -1.,  0.,  1.])\n",
            "Function f at x: tensor(215.)\n",
            "Gradient of f at x: tensor([ 306., -144.,   -2., -310.])\n",
            "tensor([ 305.9769, -143.9972,   -1.9989, -309.9976], grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}