{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"Assignment_3_notebook.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"KDVDq4R4cQqN","colab_type":"text"},"source":["Import and setup some auxiliary functions"]},{"cell_type":"code","metadata":{"id":"qHPwL1QYcQqU","colab_type":"code","colab":{}},"source":["import torch\n","from torchvision import transforms, datasets\n","import numpy as np\n","import timeit\n","from collections import OrderedDict\n","from pprint import pformat\n","from tqdm import tqdm\n","\n","torch.multiprocessing.set_sharing_strategy('file_system')\n","\n","def compute_score(acc, min_thres, max_thres):\n","    if acc <= min_thres:\n","        base_score = 0.0\n","    elif acc >= max_thres:\n","        base_score = 100.0\n","    else:\n","        base_score = float(acc - min_thres) / (max_thres - min_thres) \\\n","                     * 100\n","    return base_score\n","\n","\n","def run(algorithm, dataset_name, filename):\n","    start = timeit.default_timer()\n","    predicted_test_labels, gt_labels = algorithm(dataset_name)\n","    if predicted_test_labels is None or gt_labels is None:\n","      return (0, 0, 0)\n","    stop = timeit.default_timer()\n","    run_time = stop - start\n","    \n","    np.savetxt(filename, np.asarray(predicted_test_labels))\n","\n","    correct = 0\n","    total = 0\n","    for label, prediction in zip(gt_labels, predicted_test_labels):\n","      total += label.size(0)\n","      correct += (prediction.cpu().numpy() == label.cpu().numpy()).sum().item()   # assuming your model runs on GPU\n","      \n","    accuracy = float(correct) / total\n","    \n","    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n","    return (correct, accuracy, run_time)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c3VfLcA4cQqx","colab_type":"text"},"source":["TODO: Implement Logistic Regression here"]},{"cell_type":"code","metadata":{"id":"17Mjmw05cQq0","colab_type":"code","colab":{}},"source":["def logistic_regression(dataset_name):\n","  \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    \n","    # TODO: implement logistic regression hyper-parameter tuning here\n","    num_epochs = 5  # Number of epochs\n","    learning_rate = 1e-3  # Learning rate\n","    batch_size_training = 80  # Batch size for trainning\n","    batch_size_test = 1000 #Batch size for test\n","    \n","    \n","    # Define your train, validation, and test data loaders in PyTorch\n","    if dataset_name == \"MNIST\":\n","      input_size = 28 * 28  # MNIST dataset are 28px by 28px in size\n","      num_classes = 10  # MNIST dataset have 10 labels\n","      \n","      MNIST_training = datasets.MNIST('/MNIST_dataset/', train=True, download=True,\n","                             transform=transforms.Compose([\n","                               transforms.ToTensor(),\n","                               transforms.Normalize((0.1307,), (0.3081,))]))\n","\n","      MNIST_test_set = datasets.MNIST('/MNIST_dataset/', train=False, download=True,\n","                             transform=transforms.Compose([\n","                               transforms.ToTensor(),\n","                               transforms.Normalize((0.1307,), (0.3081,))]))\n","      \n","      # create a training and a validation set\n","      # MNIST_training_set, MNIST_validation_set = torch.utils.data.random_split(MNIST_training, [48000, 12000])\n","      # Knowing issue: I used sampler instead of random_split.  However, validation accuracy seems to drop, but validation is just for turning hyperparameter.  Since I already done that, I think is fine? The test Result stays same.\n","      train_sampler = torch.utils.data.sampler.SubsetRandomSampler(list(range(48000))) \n","      validation_sampler = torch.utils.data.sampler.SubsetRandomSampler(list(range(48000, 60000)))\n","      \n","      train_loader = torch.utils.data.DataLoader(MNIST_training,batch_size=batch_size_training, sampler=train_sampler)\n","      validation_loader = torch.utils.data.DataLoader(MNIST_training,batch_size=batch_size_training, sampler=validation_sampler)\n","      test_loader = torch.utils.data.DataLoader(MNIST_test_set,batch_size=batch_size_test, shuffle=True)\n","      \n","      test_set_len = len(MNIST_test_set)\n","      \n","    elif dataset_name == \"CIFAR10\":\n","      input_size = 32 * 32 * 3  # CIFAR10 dataset are 32px by 32px by RGB in size\n","      num_classes = 10  # CIFAR10 dataset have 10 labels\n","      \n","      transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","      \n","      CIFAR10_training = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","\n","      CIFAR10_test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","      \n","      # create a training and a validation set\n","      # CIFAR10_training_set, CIFAR10_validation_set = torch.utils.data.random_split(CIFAR10_training, [40000, 10000])\n","      # Knowing issue: I used sampler instead of random_split.  However, validation accuracy seems to drop, but validation is just for turning hyperparameter.  Since I already done that, I think is fine? The test Result stays same.\n","      train_sampler = torch.utils.data.sampler.SubsetRandomSampler(list(range(40000))) \n","      validation_sampler = torch.utils.data.sampler.SubsetRandomSampler(list(range(40000, 50000)))\n","      \n","      train_loader = torch.utils.data.DataLoader(CIFAR10_training,batch_size=batch_size_training, sampler=train_sampler, num_workers=2)\n","      validation_loader = torch.utils.data.DataLoader(CIFAR10_training,batch_size=batch_size_training, sampler=validation_sampler, num_workers=2)\n","      test_loader = torch.utils.data.DataLoader(CIFAR10_test_set,batch_size=batch_size_test, shuffle=False, num_workers=2)\n","      \n","      test_set_len = len(CIFAR10_test_set)\n","      \n","      \n","    #Define your model and cross entropy loss\n","    class LogisticRegression(torch.nn.Module):\n","      \n","      def __init__(self, input_size, num_classes):\n","        super(LogisticRegression, self).__init__()\n","        self.linear = torch.nn.Linear(input_size, num_classes)\n","        \n","      def forward(self, y):\n","        # No need to use relu, when CrossEntropyLoss do log softmax automatically\n","        # y = torch.nn.functional.relu(self.linear(y))\n","        y = self.linear(y)\n","        return y\n","    \n","    model =  LogisticRegression(input_size, num_classes) \n","    criterion = torch.nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4, amsgrad=True)   # optimize parameters weight_decay = L2\n","    \n","    # Training\n","    def train(epoch):\n","      model.train()\n","      for batch_idx, (data, target) in enumerate(train_loader):\n","        data = torch.autograd.Variable(data.view(-1, input_size))        # Images flattened into 1D tensors\n","        target = torch.autograd.Variable(target)\n","        # Forward -> Backprop -> Optimize\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = criterion(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if batch_idx % 100 == 0:\n","          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","            epoch, batch_idx * len(data), len(train_loader.dataset),\n","            100. * batch_idx / len(train_loader), loss.item()))\n","          \n","    # Validation\n","    def validation():\n","      model.eval()\n","      validation_loss = 0\n","      correct = 0\n","      with torch.no_grad(): # notice the use of no_grad\n","        for data, target in validation_loader:\n","          data = torch.autograd.Variable(data.view(-1, input_size))      # Images flattened into 1D tensors\n","          target = torch.autograd.Variable(target)\n","          output = model(data)\n","          validation_loss += criterion(output, target).item()\n","          _, pred = torch.max(output.data, 1)\n","          correct += (pred == target).sum()\n","      validation_loss /= len(validation_loader.dataset)\n","      print('\\nValidation set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(validation_loss, correct, len(validation_loader.dataset),100. * correct / len(validation_loader.dataset)))\n","    \n","    # Return Tensors\n","    predicted_test_labels = torch.zeros(test_set_len, batch_size_test)\n","    gt_labels = torch.zeros(test_set_len, batch_size_test)\n","    \n","    # Test\n","    def test():\n","      model.eval()\n","      i = 0\n","      correct = 0\n","      test_loss = 0\n","      with torch.no_grad():\n","        for data, target in test_loader:\n","          data = torch.autograd.Variable(data.view(-1, input_size))      # Images flattened into 1D tensors\n","          target = torch.autograd.Variable(target)\n","          output = model(data)\n","          test_loss += criterion(output, target).item()\n","          _, pred = torch.max(output.data, 1)\n","          correct += (pred == target).sum()\n","          \n","          # update return tensors by iterations\n","          predicted_test_labels[i:i+batch_size_test] = pred\n","          gt_labels[i:i+batch_size_test] = target.data.view_as(pred)\n","          i += batch_size_test\n","          \n","      test_loss /= len(test_loader.dataset)\n","      print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n","      \n","    # For loop, Validation after every 5 epochs\n","    validation()\n","    for epoch in range(1, num_epochs + 1):\n","      train(epoch)\n","      validation()\n","    test()\n","    \n","    #print(predicted_test_labels)\n","    #print(gt_labels)\n","    return predicted_test_labels, gt_labels"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GNNgL7C7cQq-","colab_type":"text"},"source":["Main loop. Run time and total score will be shown below."]},{"cell_type":"code","metadata":{"id":"Qf9iL8S_cQrB","colab_type":"code","outputId":"23455c0b-12f6-4ff5-982e-7e04fe9b9766","executionInfo":{"status":"ok","timestamp":1571262438461,"user_tz":360,"elapsed":118679,"user":{"displayName":"Ziming Fu","photoUrl":"","userId":"13730860658836574087"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def run_on_dataset(dataset_name, filename):\n","    if dataset_name == \"MNIST\":\n","        min_thres = 0.82\n","        max_thres = 0.92\n","\n","    elif dataset_name == \"CIFAR10\":\n","        min_thres = 0.28\n","        max_thres = 0.38\n","\n","    correct_predict, accuracy, run_time = run(logistic_regression, dataset_name, filename)\n","\n","    score = compute_score(accuracy, min_thres, max_thres)\n","    result = OrderedDict(correct_predict=correct_predict,\n","                         accuracy=accuracy, score=score,\n","                         run_time=run_time)\n","    return result, score\n","\n","\n","def main():\n","    filenames = { \"MNIST\": \"predictions_mnist_YourName_IDNumber.txt\", \"CIFAR10\": \"predictions_cifar10_YourName_IDNumber.txt\"}\n","    result_all = OrderedDict()\n","    score_weights = [0.5, 0.5]\n","    scores = []\n","    for dataset_name in [\"MNIST\",\"CIFAR10\"]:\n","        result_all[dataset_name], this_score = run_on_dataset(dataset_name, filenames[dataset_name])\n","        scores.append(this_score)\n","    total_score = [score * weight for score, weight in zip(scores, score_weights)]\n","    total_score = np.asarray(total_score).sum().item()\n","    result_all['total_score'] = total_score\n","    with open('result.txt', 'w') as f:\n","        f.writelines(pformat(result_all, indent=4))\n","    print(\"\\nResult:\\n\", pformat(result_all, indent=4))\n","\n","\n","main()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["\n","Validation set: Avg. loss: 0.0061, Accuracy: 1099/60000 (1%)\n","\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.345953\n","Train Epoch: 1 [8000/60000 (17%)]\tLoss: 0.429253\n","Train Epoch: 1 [16000/60000 (33%)]\tLoss: 0.266998\n","Train Epoch: 1 [24000/60000 (50%)]\tLoss: 0.336768\n","Train Epoch: 1 [32000/60000 (67%)]\tLoss: 0.336678\n","Train Epoch: 1 [40000/60000 (83%)]\tLoss: 0.320743\n","\n","Validation set: Avg. loss: 0.0007, Accuracy: 11010/60000 (18%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.222109\n","Train Epoch: 2 [8000/60000 (17%)]\tLoss: 0.320671\n","Train Epoch: 2 [16000/60000 (33%)]\tLoss: 0.656336\n","Train Epoch: 2 [24000/60000 (50%)]\tLoss: 0.214473\n","Train Epoch: 2 [32000/60000 (67%)]\tLoss: 0.160455\n","Train Epoch: 2 [40000/60000 (83%)]\tLoss: 0.225588\n","\n","Validation set: Avg. loss: 0.0007, Accuracy: 11062/60000 (18%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.233719\n","Train Epoch: 3 [8000/60000 (17%)]\tLoss: 0.335259\n","Train Epoch: 3 [16000/60000 (33%)]\tLoss: 0.410734\n","Train Epoch: 3 [24000/60000 (50%)]\tLoss: 0.399894\n","Train Epoch: 3 [32000/60000 (67%)]\tLoss: 0.335685\n","Train Epoch: 3 [40000/60000 (83%)]\tLoss: 0.250749\n","\n","Validation set: Avg. loss: 0.0007, Accuracy: 11024/60000 (18%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.207976\n","Train Epoch: 4 [8000/60000 (17%)]\tLoss: 0.182994\n","Train Epoch: 4 [16000/60000 (33%)]\tLoss: 0.333611\n","Train Epoch: 4 [24000/60000 (50%)]\tLoss: 0.330316\n","Train Epoch: 4 [32000/60000 (67%)]\tLoss: 0.349522\n","Train Epoch: 4 [40000/60000 (83%)]\tLoss: 0.434935\n","\n","Validation set: Avg. loss: 0.0007, Accuracy: 11069/60000 (18%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.306018\n","Train Epoch: 5 [8000/60000 (17%)]\tLoss: 0.335668\n","Train Epoch: 5 [16000/60000 (33%)]\tLoss: 0.326701\n","Train Epoch: 5 [24000/60000 (50%)]\tLoss: 0.166293\n","Train Epoch: 5 [32000/60000 (67%)]\tLoss: 0.423491\n","Train Epoch: 5 [40000/60000 (83%)]\tLoss: 0.298605\n","\n","Validation set: Avg. loss: 0.0007, Accuracy: 11071/60000 (18%)\n","\n","\n","Test set: Avg. loss: 0.0003, Accuracy: 9213/10000 (92%)\n","\n","Accuracy of the network on the 10000 test images: 92 %\n","Files already downloaded and verified\n","Files already downloaded and verified\n","\n","Validation set: Avg. loss: 0.0058, Accuracy: 1007/50000 (2%)\n","\n","Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.330942\n","Train Epoch: 1 [8000/50000 (20%)]\tLoss: 1.977003\n","Train Epoch: 1 [16000/50000 (40%)]\tLoss: 1.882228\n","Train Epoch: 1 [24000/50000 (60%)]\tLoss: 1.782815\n","Train Epoch: 1 [32000/50000 (80%)]\tLoss: 1.802717\n","\n","Validation set: Avg. loss: 0.0047, Accuracy: 3656/50000 (7%)\n","\n","Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.811638\n","Train Epoch: 2 [8000/50000 (20%)]\tLoss: 1.679261\n","Train Epoch: 2 [16000/50000 (40%)]\tLoss: 1.616353\n","Train Epoch: 2 [24000/50000 (60%)]\tLoss: 1.807434\n","Train Epoch: 2 [32000/50000 (80%)]\tLoss: 1.718717\n","\n","Validation set: Avg. loss: 0.0046, Accuracy: 3712/50000 (7%)\n","\n","Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.778162\n","Train Epoch: 3 [8000/50000 (20%)]\tLoss: 1.490430\n","Train Epoch: 3 [16000/50000 (40%)]\tLoss: 1.663450\n","Train Epoch: 3 [24000/50000 (60%)]\tLoss: 1.855416\n","Train Epoch: 3 [32000/50000 (80%)]\tLoss: 2.072866\n","\n","Validation set: Avg. loss: 0.0045, Accuracy: 3755/50000 (7%)\n","\n","Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.809118\n","Train Epoch: 4 [8000/50000 (20%)]\tLoss: 1.765270\n","Train Epoch: 4 [16000/50000 (40%)]\tLoss: 1.669642\n","Train Epoch: 4 [24000/50000 (60%)]\tLoss: 1.914052\n","Train Epoch: 4 [32000/50000 (80%)]\tLoss: 1.825746\n","\n","Validation set: Avg. loss: 0.0046, Accuracy: 3777/50000 (7%)\n","\n","Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.826889\n","Train Epoch: 5 [8000/50000 (20%)]\tLoss: 1.474393\n","Train Epoch: 5 [16000/50000 (40%)]\tLoss: 1.636353\n","Train Epoch: 5 [24000/50000 (60%)]\tLoss: 1.730299\n","Train Epoch: 5 [32000/50000 (80%)]\tLoss: 1.730775\n","\n","Validation set: Avg. loss: 0.0045, Accuracy: 3786/50000 (7%)\n","\n","\n","Test set: Avg. loss: 0.0018, Accuracy: 3900/10000 (39%)\n","\n","Accuracy of the network on the 10000 test images: 39 %\n","\n","Result:\n"," OrderedDict([   (   'MNIST',\n","                    OrderedDict([   ('correct_predict', 9213000),\n","                                    ('accuracy', 0.9213),\n","                                    ('score', 100.0),\n","                                    ('run_time', 55.44701469099999)])),\n","                (   'CIFAR10',\n","                    OrderedDict([   ('correct_predict', 3900000),\n","                                    ('accuracy', 0.39),\n","                                    ('score', 100.0),\n","                                    ('run_time', 52.04276858100002)])),\n","                ('total_score', 100.0)])\n"],"name":"stdout"}]}]}